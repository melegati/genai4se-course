{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsYxqp_LCfG7"
      },
      "source": [
        "# Fine-tuning CodeBERT for vulnerability detection\n",
        "\n",
        "In this notebook, we provide an example of fine-tuning CodeBERT for vulnerability detection.\n",
        "\n",
        "Each input will consist of a function and the corresponding output will be if the function contains a vulnerability or not.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will use the subset of the Devign [1] dataset which is publicly available (only the projects FFmpeg and Qemu) through the MADE-WIC [2] fused dataset.\n",
        "\n",
        "The following scripts download MADE-WIC, extract Devign from it and remove the remaining.\n",
        "\n",
        "[1] Y. Zhou, S. Liu, J. Siow, X. Du, and Y. Liu, “Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks,” in Advances in Neural Information Processing Systems, 2019, vol. 32. [Online]. Available: https://sites.google.com/view/devign\n",
        "\n",
        "[2] M. Mock, J. Melegati, M. Kretschmann, N. E. Diaz Ferreyra, and B. Russo, “MADE-WIC: Multiple Annotated Datasets for Exploring Weaknesses In Code,” in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, Oct. 2024, pp. 2346–2349. doi: 10.1145/3691620.3695348.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efjo4AYK_aq"
      },
      "outputs": [],
      "source": [
        "!wget -O MADE-WIC.zip https://zenodo.org/records/13370805/files/MADE-WIC.zip?download=1\n",
        "!unzip -j MADE-WIC.zip \"MADE-WIC/Dataset/devign/*\" -d devign\n",
        "!rm MADE-WIC.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QTTM9d7CfG-"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "In this notebook we will use Keras since it describes in a higher-level the layers, facilitating the comprehension of the code. As backend, we will use Tensorflow.\n",
        "\n",
        "We will also use transformers to take advantage of the pre-trained models available in HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV0CtLhCIKr-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFRobertaModel, RobertaTokenizer\n",
        "from tf_keras import Model\n",
        "from tf_keras.layers import Dense, Input, Dropout\n",
        "from tf_keras.regularizers import L2\n",
        "from tf_keras.metrics import Precision, Recall\n",
        "from tf_keras.losses import BinaryCrossentropy\n",
        "from tf_keras.optimizers import AdamW\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRuicQjqCfG_"
      },
      "source": [
        "### Checking access to the hardware\n",
        "\n",
        "Let's check if the configuration is correct and we have access to a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlA-k9rECfG_"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsQ-dqhLCfHA"
      },
      "source": [
        "## Model\n",
        "\n",
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4wxtZKOJSHe"
      },
      "outputs": [],
      "source": [
        "dropout_prob = 0.1\n",
        "l2_reg_lambda = 0.2\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "max_length = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTBaQNECfHB"
      },
      "source": [
        "### Architecture\n",
        "\n",
        "First we load the pre-trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rFkqOHMCfHC"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"huggingface/CodeBERTa-small-v1\") # If you have computing power, you can use microsoft/codebert-base\n",
        "model = TFRobertaModel.from_pretrained(\"huggingface/CodeBERTa-small-v1\") # If you have computing power, you can use microsoft/codebert-base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_pW03PCfHC"
      },
      "source": [
        "Then, we define the input layer (this model requires an attention mask which is used to tell to the model was is not padding the input) and pass it to the loaded model.\n",
        "From the model, we get the hidden state of the first token ([CLS]) from the last layer.\n",
        "We add a dropout layer (which randomly \"turns off\" some neurons) since it usually improves the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdB4m2P9CfHD"
      },
      "outputs": [],
      "source": [
        "input_ids = Input(shape=(512, ), dtype='int32', name='input_ids')\n",
        "attention_mask = Input(shape=(512, ), dtype='int32', name='attention_mask')\n",
        "model = model([input_ids, attention_mask])\n",
        "embedding = model.last_hidden_state[:, 0, :]\n",
        "embedding = Dropout(dropout_prob)(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZvf3siWCfHD"
      },
      "source": [
        "We then connect all these outputs to the output layer consisting of a single neuron."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xo5jpzzCfHD"
      },
      "outputs": [],
      "source": [
        "output = Dense(1,\n",
        "                kernel_initializer='glorot_normal',\n",
        "                kernel_regularizer=L2(l2_reg_lambda),\n",
        "                bias_regularizer=L2(l2_reg_lambda),\n",
        "                activation='sigmoid')(embedding)\n",
        "\n",
        "model = Model(inputs=[input_ids, attention_mask], outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwNC7kDfCfHD"
      },
      "source": [
        "Then, we compile the model describing what will be the loss function during the training, the optimizer to be used and the metrics to monitor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5deIEvMCfHE"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=BinaryCrossentropy(),\n",
        "              optimizer=AdamW(learning_rate),\n",
        "              metrics=['accuracy', Precision(), Recall()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xpbc822CfHE"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "Let's now load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJcyJpc4OKNn"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('devign/complete.csv')\n",
        "\n",
        "#df.fillna(value='', inplace=True)\n",
        "#df.replace(to_replace=[None], value='', inplace=True)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((df['Function'], df['Devign']))\n",
        "#dataset = dataset.take(1000)\n",
        "num_samples = len(dataset)\n",
        "\n",
        "print('Samples in dataset:', num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a fraction of the dataset so it can finish faster. If you have a powerful hardware, you can skip the next cell."
      ],
      "metadata": {
        "id": "F1iIcMPfF3WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.take(int(int(num_samples) * 0.1))\n",
        "num_samples = len(dataset)\n",
        "\n",
        "print('Samples in dataset:', num_samples)"
      ],
      "metadata": {
        "id": "sjhcIPTuHHfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " And split it into train, validation and testing."
      ],
      "metadata": {
        "id": "u_D2AP-JHctJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(int(num_samples * 0.8))\n",
        "validation_ds = dataset.skip(int(num_samples * 0.8)).take(int(num_samples * 0.1))\n",
        "test_ds = dataset.skip(int(num_samples * 0.9))\n",
        "\n",
        "print('Samples in train dataset:', len(train_ds))\n",
        "print('Samples in validation dataset:', len(validation_ds))\n",
        "print('Samples in test dataset:', len(test_ds))"
      ],
      "metadata": {
        "id": "GitBOQ-tF4TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFQV49kRCfHE"
      },
      "source": [
        "Since the implementation provided does not have the tokenizer inside the model but rather as a separate class. Let's prepare the inputs to be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAhLJExSUm4i"
      },
      "outputs": [],
      "source": [
        "def encode_examples(tokenizer, ds):\n",
        "    # Prepare Input list\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for code, vulnerable in tfds.as_numpy(ds):\n",
        "        bert_input = tokenizer.encode_plus(code.decode(),\n",
        "                                        add_special_tokens=True,\n",
        "                                        max_length=max_length,\n",
        "                                        padding='max_length',\n",
        "                                        return_attention_mask=True,\n",
        "                                        truncation=True\n",
        "                                        )\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append(vulnerable)\n",
        "\n",
        "    return { 'input_ids':  tf.convert_to_tensor(input_ids_list),\n",
        "              'attention_mask': tf.convert_to_tensor(attention_mask_list) }, tf.convert_to_tensor(label_list)\n",
        "\n",
        "train_ds_encoded, train_labels = encode_examples(tokenizer, train_ds)\n",
        "validation_ds_encoded, validation_labels = encode_examples(tokenizer, validation_ds)\n",
        "test_ds_encoded, test_labels = encode_examples(tokenizer, test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPK51yzCfHF"
      },
      "source": [
        "## Training\n",
        "\n",
        "For the training, we provide the number of epochs to run and the batch size besides training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrTt-qjqmt5I"
      },
      "outputs": [],
      "source": [
        "model.fit(train_ds_encoded,\n",
        "          train_labels,\n",
        "          epochs=num_epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(validation_ds_encoded, validation_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRKXHuaCfHF"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's use the trained model to predict the existence of vulnerabilities for the functions in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAe1hEL1o1qm"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_ds_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fMLoh3bCfHG"
      },
      "source": [
        "Now, let's calculate the performance scores. The output is a number in the interval [0 1]. So we consider any value above 0.5 as an indication of the existence of a vulnerability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrpH4PUJGz8S"
      },
      "outputs": [],
      "source": [
        "def calculate_scores(predictions, label):\n",
        "\n",
        "    if hasattr(label, \"ndim\") and label.ndim > 1:\n",
        "        label = label.squeeze()\n",
        "\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "\n",
        "    for index in range(len(predictions)):\n",
        "        prediction = predictions[index] if isinstance(predictions[index], bool) else predictions[index][0] > 0.5\n",
        "\n",
        "        if(label[index] == True):\n",
        "            if(prediction == True):\n",
        "                tp = tp + 1\n",
        "            else:\n",
        "                fn = fn + 1\n",
        "        else:\n",
        "            if(prediction == False):\n",
        "                tn = tn + 1\n",
        "            else:\n",
        "                fp = fp + 1\n",
        "\n",
        "    print(\"TP:\", tp)\n",
        "    print(\"TN:\", tn)\n",
        "    print(\"FP:\", fp)\n",
        "    print(\"FN:\", fn)\n",
        "\n",
        "    precision = tp / (tp + fp) if tp + fp > 0 else -1\n",
        "    recall = tp / (tp + fn) if tp + fn > 0 else -1\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    f1 = 2 * ((precision * recall) / (precision + recall)) if precision + recall > 0 else -1\n",
        "\n",
        "    print(\"\\nPrecision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"F1:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUHWqWL-HsCO"
      },
      "outputs": [],
      "source": [
        "calculate_scores(predictions, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqTKZ6h9CfHG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}