{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53baa48b-2530-4fa1-9cd0-c9ecab806a99",
   "metadata": {
    "id": "53baa48b-2530-4fa1-9cd0-c9ecab806a99"
   },
   "source": [
    "# Retrieval-augmented Generation (RAG)\n",
    "\n",
    "In this example, we will use RAG to generate code based on an external API defined in a Swagger file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c14dd-2053-49ba-ac76-f7b622bbb745",
   "metadata": {
    "id": "517c14dd-2053-49ba-ac76-f7b622bbb745"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "### LangChain\n",
    "\n",
    "Here, we will use the LangChain libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab694e-5cea-4853-b0d8-9e2fc5434ea2",
   "metadata": {
    "id": "8cab694e-5cea-4853-b0d8-9e2fc5434ea2"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install langchain jq langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffcc4f-64c8-4b21-ae3f-4b10ee018cfd",
   "metadata": {
    "id": "b1ffcc4f-64c8-4b21-ae3f-4b10ee018cfd"
   },
   "source": [
    "### Model\n",
    "\n",
    "We will use OpenAI's models, so we need an API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426e6f6-43e1-4637-9900-6bde98ef1d60",
   "metadata": {
    "id": "3426e6f6-43e1-4637-9900-6bde98ef1d60"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mHKouQuQ7KB9",
   "metadata": {
    "id": "mHKouQuQ7KB9"
   },
   "source": [
    "# Target\n",
    "\n",
    "Let's work with the OpenDataHub API to get tourism data from South Tyrol.\n",
    "\n",
    "To do that, let's download the JSON file with the definitions of the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CTasn9hn7cHi",
   "metadata": {
    "id": "CTasn9hn7cHi"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/melegati/genai4se-course/refs/heads/master/opendatahub.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5cb2e6-5460-4f06-925a-f3853ff44e16",
   "metadata": {
    "id": "8e5cb2e6-5460-4f06-925a-f3853ff44e16"
   },
   "source": [
    "## Retrieval\n",
    "\n",
    "First, we have to load the documents that will be stored in the database.\n",
    "\n",
    "To this aim, we will read the JSON file describing the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc183618-6a17-42fb-8381-1b72544e96d3",
   "metadata": {
    "id": "dc183618-6a17-42fb-8381-1b72544e96d3"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "from pathlib import Path\n",
    "import jq\n",
    "\n",
    "file_path='./opendatahub.json'\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = json.load(f)\n",
    "    api_url = jq.compile('.servers[0].url').input(data).first()\n",
    "    print(api_url)\n",
    "\n",
    "loader = JSONLoader(\n",
    "         file_path=file_path,\n",
    "         jq_schema='.paths | to_entries[] | .key as $path | .value | to_entries[] | { path:$path, method:.key, tag:.value.tags[0], summary:.value.summary, parameters: [ {name: .value.parameters[]?.name } ] }',\n",
    "         text_content=False)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883fd6c-e9fa-4f88-b0b0-b03ca110e05b",
   "metadata": {
    "id": "2883fd6c-e9fa-4f88-b0b0-b03ca110e05b"
   },
   "source": [
    "Checking the number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86101c3b-c6e7-4e64-9397-35aad2b42639",
   "metadata": {
    "id": "86101c3b-c6e7-4e64-9397-35aad2b42639"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95943838-cd2f-4538-acfc-3a9b578ac52d",
   "metadata": {
    "id": "95943838-cd2f-4538-acfc-3a9b578ac52d"
   },
   "source": [
    "Let's check the first document to take a look on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c863df-c679-419e-9439-7a16b2232836",
   "metadata": {
    "id": "41c863df-c679-419e-9439-7a16b2232836"
   },
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77f7e9-d706-43a3-9f09-6950ddb0a1ac",
   "metadata": {
    "id": "bb77f7e9-d706-43a3-9f09-6950ddb0a1ac"
   },
   "source": [
    "Let's use OpenAI's text-embedding-3-large model for creating the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6d94-584a-4990-8d83-7ba39bd69203",
   "metadata": {
    "id": "c66b6d94-584a-4990-8d83-7ba39bd69203"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa7276-cc25-43c4-8e4b-5ae1d38fd4cf",
   "metadata": {
    "id": "4baa7276-cc25-43c4-8e4b-5ae1d38fd4cf"
   },
   "source": [
    "For this didactic example, we will use the LangChain's InMemoryVectorStore. According to the [documentation](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html), it uses a dictionary and the similarity is calculated using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554795e1-c45a-48ad-8feb-660c04443d0f",
   "metadata": {
    "id": "554795e1-c45a-48ad-8feb-660c04443d0f"
   },
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca9e8d-c92d-482e-bd20-2ecefd2ee6db",
   "metadata": {
    "id": "66ca9e8d-c92d-482e-bd20-2ecefd2ee6db"
   },
   "source": [
    "Now let's add the documents to the vector store. In this process, the embeddings are calculated using the defined model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfeb4f-ea9f-4251-a487-29131e767719",
   "metadata": {
    "id": "f0dfeb4f-ea9f-4251-a487-29131e767719"
   },
   "outputs": [],
   "source": [
    "vector_store.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d08e47-1e5b-43dd-b932-b6bf7bd251d3",
   "metadata": {
    "id": "d6d08e47-1e5b-43dd-b932-b6bf7bd251d3"
   },
   "source": [
    "Let's create a function to retrieve the relevant documents given the text definition of a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f03c13-9c08-448d-9794-2ea08651e919",
   "metadata": {
    "id": "05f03c13-9c08-448d-9794-2ea08651e919"
   },
   "outputs": [],
   "source": [
    "def retrieve(task):\n",
    "    retrieved_docs = vector_store.similarity_search(task)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccf430-23e0-4384-9383-879e4273c228",
   "metadata": {
    "id": "7fccf430-23e0-4384-9383-879e4273c228"
   },
   "source": [
    "Let's check if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e34dca-76b5-4c31-82b9-e7d2e9baac6a",
   "metadata": {
    "id": "88e34dca-76b5-4c31-82b9-e7d2e9baac6a"
   },
   "outputs": [],
   "source": [
    "task = \"Write a piece of code to list the events in the last year.\"\n",
    "retrieve(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd480e-3404-459f-bdf2-fde9a99d0c23",
   "metadata": {
    "id": "dabd480e-3404-459f-bdf2-fde9a99d0c23"
   },
   "source": [
    "## Generation\n",
    "\n",
    "Let's use OpenAI's GPT-4o-mini to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3659189-0779-4b77-8c91-b161673e2851",
   "metadata": {
    "id": "d3659189-0779-4b77-8c91-b161673e2851"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6523b70-c4da-4b6b-aff6-d25c6cac820b",
   "metadata": {
    "id": "d6523b70-c4da-4b6b-aff6-d25c6cac820b"
   },
   "source": [
    "Here we define the prompt template, leaving the space for information retrieved about the API and the task. We also add the information about the API url so the generated code will be runnable. Let's save the result in a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2989fc5-096c-48b8-a77d-43b4779873d9",
   "metadata": {
    "id": "e2989fc5-096c-48b8-a77d-43b4779873d9"
   },
   "outputs": [],
   "source": [
    "def generate(task, api_info, api_url):\n",
    "\n",
    "    prompt_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a developer using an API to implement a solution in Python.\"),\n",
    "        (\"user\", \"Below, there is the information about the API you need to use {apiInfo}. Your task is: {task}. Just return the code without anything else. The API URL is {apiUrl}\")\n",
    "    ])\n",
    "\n",
    "    prompt = prompt_template.invoke({\"apiInfo\": api_info, \"task\": task, \"apiUrl\": api_url})\n",
    "    answer = llm.invoke(prompt)\n",
    "    result = answer.content\n",
    "\n",
    "    answer.pretty_print()\n",
    "\n",
    "    if \"```python\" in result:\n",
    "            result = result[10:-3]\n",
    "\n",
    "    output_path = \"output/main.py\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mwhk5UM18PFr",
   "metadata": {
    "id": "mwhk5UM18PFr"
   },
   "source": [
    "We need a folder to save the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9JwMCQ88SFw",
   "metadata": {
    "id": "v9JwMCQ88SFw"
   },
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58ab74-5780-4686-add3-9c01b57a62e0",
   "metadata": {
    "id": "ae58ab74-5780-4686-add3-9c01b57a62e0"
   },
   "outputs": [],
   "source": [
    "api_info = retrieve(task)\n",
    "generate(task, api_info, api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MgD3mdq_8d3j",
   "metadata": {
    "id": "MgD3mdq_8d3j"
   },
   "source": [
    "Let's execute the generate code to check if it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857caf8-ca29-4008-8a32-8d6d43e83574",
   "metadata": {
    "id": "3857caf8-ca29-4008-8a32-8d6d43e83574"
   },
   "outputs": [],
   "source": [
    "!python output/main.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
